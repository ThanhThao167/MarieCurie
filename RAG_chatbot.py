# RAG_chatbot.py - OPTIMIZED VERSION
# FastAPI backend cho Chatbot Tuy·ªÉn sinh 10 ‚Äì THPT Marie Curie (Cloud-ready)
# T·ªëi ∆∞u: Gi·∫£m top_k, ng∆∞·ª°ng similarity, context window GPT

import os
import csv
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from deep_translator import GoogleTranslator
from dotenv import load_dotenv
from fastapi import FastAPI, Request, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel
from openai import OpenAI

# =========================
# 0) ENV & Logging
# =========================
load_dotenv()
logger = logging.getLogger("RAG_Chatbot")
logging.basicConfig(level=logging.INFO)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.warning("‚ö†Ô∏è OPENAI_API_KEY ch∆∞a ƒë∆∞·ª£c ƒë·∫∑t ‚Äì fallback GPT s·∫Ω l·ªói.")
client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

# Th∆∞ m·ª•c l∆∞u d·ªØ li·ªáu CSV (c√≥ th·ªÉ ƒë·ªïi qua ENV: DATA_DIR), m·∫∑c ƒë·ªãnh l√† root
DATA_DIR = Path(os.getenv("DATA_DIR", "."))
DATA_DIR.mkdir(parents=True, exist_ok=True)
CHAT_CSV = DATA_DIR / "chat_history.csv"
FEED_CSV = DATA_DIR / "feedback.csv"
KB_CSV = DATA_DIR / "MC_chatbot.csv"   # ngu·ªìn tri th·ª©c

# =========================
# 1) App & CORS
# =========================
app = FastAPI(title="Marie Curie RAG Chatbot API")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"]
)

# =========================
# 2) Pydantic payload
# =========================
class ChatPayload(BaseModel):
    session_id: str
    messages: List[Dict[str, Any]]  # [{"role":"user/assistant","content":"..."}]

# =========================
# 3) Load Knowledge Base + FAISS
# =========================
def _load_kb(kb_path: Path) -> tuple[list[str], list[str]]:
    if not kb_path.exists():
        logger.warning(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y {kb_path}. RAG s·∫Ω ho·∫°t ƒë·ªông h·∫°n ch·∫ø.")
        return [], []
    df = pd.read_csv(kb_path, encoding="utf-8")
    # b·∫Øt bu·ªôc c√≥ 2 c·ªôt 'cauhoi' v√† 'cautraloi'
    if not {"cauhoi", "cautraloi"}.issubset(df.columns):
        raise RuntimeError("File MC_chatbot.csv c·∫ßn c√≥ c·ªôt 'cauhoi' v√† 'cautraloi'.")
    return df["cauhoi"].tolist(), df["cautraloi"].tolist()

questions, answers = _load_kb(KB_CSV)

# Model & FAISS index
logger.info("üì¶ ƒêang load SentenceTransformer model...")
model = SentenceTransformer("all-MiniLM-L6-v2")
if questions:
    logger.info(f"üî® ƒêang t·∫°o FAISS index cho {len(questions)} c√¢u h·ªèi...")
    embeddings = model.encode(questions, convert_to_numpy=True, show_progress_bar=False)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    logger.info("‚úÖ FAISS index ƒë√£ s·∫µn s√†ng!")
else:
    embeddings = None
    index = None
    logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ c√¢u h·ªèi n√†o trong KB!")

# =========================
# 4) CSV helpers
# =========================
def _append_csv(path: Path, fieldnames: list[str], row: dict):
    path.parent.mkdir(parents=True, exist_ok=True)
    file_exists = path.exists()
    with path.open("a", newline="", encoding="utf-8") as f:
        w = csv.DictWriter(f, fieldnames=fieldnames)
        if not file_exists:
            w.writeheader()
        w.writerow({k: row.get(k, "") for k in fieldnames})

def _read_csv_dicts(path: Path) -> list[dict]:
    if not path.exists():
        return []
    with path.open("r", newline="", encoding="utf-8") as f:
        return list(csv.DictReader(f))

def save_chat(session_id: str, role: str, content: str):
    _append_csv(
        CHAT_CSV,
        ["timestamp", "session_id", "role", "content"],
        {"timestamp": datetime.utcnow().isoformat(), "session_id": session_id, "role": role, "content": content},
    )

def save_feedback(session_id: str, question: str, answer: str, rating: str):
    _append_csv(
        FEED_CSV,
        ["timestamp", "session_id", "question", "answer", "rating"],
        {
            "timestamp": datetime.utcnow().isoformat(),
            "session_id": session_id,
            "question": question,
            "answer": answer,
            "rating": rating,
        },
    )

# =========================
# 5) Ti·ªán √≠ch ng√¥n ng·ªØ & ng√†y th√°ng
# =========================
def detect_language(text: str) -> str:
    en_keywords = ["what", "how", "where", "when", "school", "admission", "who", "principal", "gold", "weather", "tomorrow"]
    return "en" if any(w in text.lower() for w in en_keywords) else "vi"

DAY_MAP = {
    "Monday": "Hai", "Tuesday": "Ba", "Wednesday": "T∆∞",
    "Thursday": "NƒÉm", "Friday": "S√°u", "Saturday": "B·∫£y", "Sunday": "Ch·ªß Nh·∫≠t"
}

def get_date_info(days_offset=0) -> str:
    now = datetime.now() + timedelta(days=days_offset)
    day_vi = DAY_MAP.get(now.strftime("%A"), now.strftime("%A"))
    prefix = "H√¥m nay" if days_offset == 0 else "Ng√†y mai" if days_offset == 1 else "H√¥m qua" if days_offset == -1 else f"Ng√†y {now.day} th√°ng {now.month} nƒÉm {now.year}"
    return f"{prefix} l√† th·ª© {day_vi}, ng√†y {now.day} th√°ng {now.month} nƒÉm {now.year}."

# =========================
# 6) RAG ‚Äì Retrieve (OPTIMIZED)
# =========================
def retrieve_context(query: str, top_k: int = 2) -> tuple[list[str], float]:
    """
    OPTIMIZED: Gi·∫£m top_k t·ª´ 3 ‚Üí 2 ƒë·ªÉ tƒÉng t·ªëc
    """
    if index is None or embeddings is None or not questions:
        return [], 0.0
    
    q_embed = model.encode([query], convert_to_numpy=True, show_progress_bar=False)
    D, I = index.search(q_embed, top_k)
    
    # L·ªçc c√°c k·∫øt qu·∫£ qu√° xa (L2 nh·ªè h∆°n ng∆∞·ª°ng th√¨ g·∫ßn)
    # TƒÉng ng∆∞·ª°ng l√™n 1.2 ƒë·ªÉ ch·∫•p nh·∫≠n k·∫øt qu·∫£ r·ªông h∆°n
    contexts = [
        f"Q: {questions[i]}\nA: {answers[i]}\n"
        for idx, i in enumerate(I[0])
        if i >= 0 and D[0][idx] < 1.2
    ]
    
    # Similarity "gi·∫£ l·∫≠p" t·ª´ L2 distance
    best_sim = float(max(0.0, 1.0 - float(D[0][0]))) if len(D[0]) > 0 else 0.0
    
    return contexts, best_sim

# =========================
# 7) API
# =========================
@app.get("/health")
def health():
    return {
        "status": "ok", 
        "kb_loaded": len(questions) if questions else 0,
        "faiss_ready": index is not None
    }

@app.post("/chat")
async def chat_handler(payload: ChatPayload, request: Request):
    """
    OPTIMIZED: 
    - Gi·∫£m ng∆∞·ª°ng similarity t·ª´ 0.85 ‚Üí 0.75
    - Gi·∫£m context window GPT t·ª´ -3 ‚Üí -2
    """
    user_input = payload.messages[-1]["content"]
    session_id = payload.session_id
    user_lang = detect_language(user_input)

    # D·ªãch sang ti·∫øng Vi·ªát n·∫øu ng∆∞·ªùi d√πng n√≥i ti·∫øng Anh (ƒë·ªÉ retrieve t·ªët h∆°n)
    try:
        translated_input = GoogleTranslator(source="auto", target="vi").translate(user_input) if user_lang == "en" else user_input
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Translation failed: {e}")
        translated_input = user_input

    # Retrieve tri th·ª©c
    context_chunks, similarity = retrieve_context(translated_input)
    logger.info(f"üîç FAISS similarity = {similarity:.2f} | Found {len(context_chunks)} contexts")

    # L∆∞u c√¢u c·ªßa user
    save_chat(session_id, "user", user_input)

    lower_input = translated_input.lower()
    
    # X·ª≠ l√Ω ƒë·∫∑c bi·ªát v·ªÅ ng√†y th√°ng (real-time info)
    if "h√¥m nay" in lower_input or "hi·ªán t·∫°i" in lower_input or "th·ª© m·∫•y h√¥m nay" in lower_input:
        reply = get_date_info(0)
    elif "ng√†y mai" in lower_input or "mai l√† th·ª© m·∫•y" in lower_input:
        reply = get_date_info(1)
    elif "h√¥m qua" in lower_input or "qua l√† th·ª© m·∫•y" in lower_input:
        reply = get_date_info(-1)
    else:
        reply = None

    if reply is not None:
        if user_lang == "en":
            try:
                reply = GoogleTranslator(source="vi", target="en").translate(reply)
            except Exception:
                pass
        save_chat(session_id, "assistant", reply)
        return {"response": reply, "source": "real_time", "similarity": round(float(similarity), 2)}

    # OPTIMIZED: Gi·∫£m ng∆∞·ª°ng similarity t·ª´ 0.85 ‚Üí 0.75
    if similarity >= 0.75 and context_chunks:
        logger.info("‚úÖ S·ª≠ d·ª•ng c√¢u tr·∫£ l·ªùi t·ª´ Knowledge Base")
        top_answer = context_chunks[0].split("A:", 1)[-1].strip()
        if user_lang == "en":
            try:
                top_answer = GoogleTranslator(source="vi", target="en").translate(top_answer)
            except Exception:
                pass
        save_chat(session_id, "assistant", top_answer)
        return {"response": top_answer, "source": "knowledge_base", "similarity": round(float(similarity), 2)}

    # Fallback GPT
    logger.info("ü§ñ Calling GPT-4o-mini as fallback...")
    current_date_info = get_date_info(0)
    
    prompt = (
        f"{current_date_info}\n\n"
        "B·∫°n l√† tr·ª£ l√Ω AI th√¢n thi·ªán, ch√≠nh x√°c c·ªßa THPT Marie Curie. "
        "Tr·∫£ l·ªùi b·∫±ng c√πng ng√¥n ng·ªØ v·ªõi ng∆∞·ªùi d√πng (Anh/Vi·ªát). "
        "N·∫øu c√≥ ng·ªØ c·∫£nh n·ªôi b·ªô, ∆∞u ti√™n d√πng.\n\n"
        + ("\n".join(context_chunks) if context_chunks else "")
    )
    
    # OPTIMIZED: Gi·∫£m context window t·ª´ -3 ‚Üí -2 messages
    messages = [{"role": "system", "content": prompt}] + payload.messages[-2:]

    try:
        if client is None:
            raise RuntimeError("OPENAI_API_KEY not set")
        
        completion = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=300,  # Gi·ªõi h·∫°n ƒë·ªô d√†i response
            temperature=0.7
        )
        gpt_reply = completion.choices[0].message.content
        logger.info("‚úÖ GPT response received")
        
    except Exception as e:
        logger.error(f"‚ùå GPT error: {e}")
        gpt_reply = (
            "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë khi t·∫°o c√¢u tr·∫£ l·ªùi." 
            if user_lang == "vi" 
            else "Sorry, the system is experiencing an issue generating the response."
        )

    save_chat(session_id, "assistant", gpt_reply)
    return {"response": gpt_reply, "source": "rag_gpt", "similarity": round(float(similarity), 2)}

# -------------------------
# 8) Feedback
# -------------------------
@app.post("/feedback")
async def feedback(
    session_id: str = Form(...),
    question: str = Form(...),
    answer: str = Form(...),
    rating: str = Form(...)
):
    save_feedback(session_id, question, answer, rating)
    logger.info(f"üìù Feedback received: {rating} | Session: {session_id}")
    return {"status": "ok", "message": "ƒê√£ ghi nh·∫≠n ph·∫£n h·ªìi."}

# -------------------------
# 9) Endpoints d·ªØ li·ªáu cho trang Qu·∫£n tr·ªã
# -------------------------
@app.get("/history")
def get_history():
    """Tr·∫£ JSON l·ªãch s·ª≠ chat."""
    return JSONResponse(_read_csv_dicts(CHAT_CSV))

@app.get("/feedbacks")
def get_feedbacks():
    """Tr·∫£ JSON danh s√°ch feedback."""
    return JSONResponse(_read_csv_dicts(FEED_CSV))

@app.get("/chat_history.csv")
def download_history_csv():
    if not CHAT_CSV.exists():
        return JSONResponse({"detail": "chat_history.csv not found"}, status_code=404)
    return FileResponse(CHAT_CSV, media_type="text/csv", filename="chat_history.csv")

@app.get("/feedback.csv")
def download_feedback_csv():
    if not FEED_CSV.exists():
        return JSONResponse({"detail": "feedback.csv not found"}, status_code=404)
    return FileResponse(FEED_CSV, media_type="text/csv", filename="feedback.csv")

# -------------------------
# 10) Endpoint n·∫°p l·∫°i KB khi thay MC_chatbot.csv
# -------------------------
@app.post("/reload_kb")
def reload_kb():
    """ƒê·ªçc l·∫°i MC_chatbot.csv & t√°i t·∫°o FAISS m√† kh√¥ng c·∫ßn redeploy."""
    global questions, answers, embeddings, index
    
    logger.info("üîÑ Reloading Knowledge Base...")
    q, a = _load_kb(KB_CSV)
    
    if not q:
        return JSONResponse({"detail": "MC_chatbot.csv missing or invalid"}, status_code=400)

    questions, answers = q, a
    embeddings = model.encode(questions, convert_to_numpy=True, show_progress_bar=False)
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    
    logger.info(f"‚úÖ KB reloaded successfully: {len(questions)} questions")
    return {"status": "ok", "count": len(questions)}

# -------------------------
# 11) Optional: Upload endpoint (n·∫øu c·∫ßn upload MC_chatbot.csv qua API)
# -------------------------
from fastapi import UploadFile, File

@app.post("/upload_mc_data")
async def upload_mc_data(file: UploadFile = File(...)):
    """Upload file MC_chatbot.csv m·ªõi v√† reload KB."""
    if not file.filename.endswith('.csv'):
        return JSONResponse({"detail": "Ch·ªâ ch·∫•p nh·∫≠n file .csv"}, status_code=400)
    
    try:
        content = await file.read()
        KB_CSV.write_bytes(content)
        logger.info(f"üì§ Uploaded new KB file: {KB_CSV}")
        
        # Auto reload
        result = reload_kb()
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Upload failed: {e}")
        return JSONResponse({"detail": str(e)}, status_code=500)

# -------------------------
# Startup event
# -------------------------
@app.on_event("startup")
async def startup_event():
    logger.info("=" * 60)
    logger.info("üöÄ Marie Curie RAG Chatbot API Starting...")
    logger.info(f"üìö Knowledge Base: {len(questions)} questions loaded")
    logger.info(f"üîß FAISS Index: {'Ready' if index else 'Not Ready'}")
    logger.info(f"ü§ñ OpenAI API: {'Configured' if client else 'Missing'}")
    logger.info("=" * 60)
